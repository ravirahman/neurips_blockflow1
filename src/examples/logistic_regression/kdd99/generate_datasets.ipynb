{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Union\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from examples.logistic_regression.kdd99.dataset import kdd99_csv_to_dataframe, kdd99_df_to_dataset, ATTACK_CLASSIFICATIONS\n",
    "from examples.logistic_regression.model import LogisticRegressionTrainingModel, LogisticRegressionTrainingModelParams, LogisticRegressionEvaluationModelParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fake:  # wrapper to denote to generate data that is representative but not necessarily correct\n",
    "    def __init__(self, fraction, *, attack_type: str):\n",
    "        self.fraction = fraction\n",
    "        self.attack_type = attack_type\n",
    "\n",
    "def generate_rel_split(num_special_agents: int, num_normal_agents: int, special_agent_factor: float):\n",
    "    denom = num_special_agents * special_agent_factor + num_normal_agents\n",
    "    answer = []\n",
    "    for _ in range(num_special_agents):\n",
    "        answer.append(special_agent_factor/denom)\n",
    "    for _ in range(num_normal_agents):\n",
    "        answer.append(1/denom)\n",
    "    return answer\n",
    "\n",
    "# dict of names to split sequences.\n",
    "# each subsequence is a sequence of tuples representing what % of the (false data, true data) each client should have. True data and false data %s must each sum to 1.0\n",
    "SPLITS = {\n",
    "    # symmetric and equal splits\n",
    "    \"sym_eq_1\": (1.0,),  # one client with everything\n",
    "    \"sym_eq_25\": [1/25] * 25,  # 25 clients with equal splits\n",
    "    \"sym_eq_50\": [1/50] * 50,  # 50 clients with equal splits\n",
    "    \"sym_eq_100\": [1/100] * 100,  # 50 clients with equal splits\n",
    "    \n",
    "    \"sym_neq_50x0.125\": generate_rel_split(10, 40, .125),\n",
    "    \"sym_neq_50x0.25\": generate_rel_split(10, 40, .25),\n",
    "    \"sym_neq_50x0.5\": generate_rel_split(10, 40, .5),\n",
    "    \"sym_neq_50x0.75\": generate_rel_split(10, 40, .75),\n",
    "    \"sym_neq_50x1.5\": generate_rel_split(10, 40, 1.5),\n",
    "    \"sym_neq_50x2\": generate_rel_split(10, 40, 2),\n",
    "    \"sym_neq_50x4\": generate_rel_split(10, 40, 4),\n",
    "\n",
    "    # attack splits\n",
    "    \"attack_random_50_one\":       [1/50] * 49 + [Fake(1/50, attack_type='random')],\n",
    "    \"attack_random_50_quarter\":   [1/50] * 38 + [Fake(1/50, attack_type='random')] * 12,\n",
    "    \"attack_random_50_half\":      [1/50] * 26 + [Fake(1/50, attack_type='random')] * 24,\n",
    "    \"attack_inverted_50_one\":     [1/50] * 49 + [Fake(1/50, attack_type='inverted')],\n",
    "    \"attack_inverted_50_quarter\": [1/50] * 38 + [Fake(1/50, attack_type='inverted')] * 12,\n",
    "    \"attack_inverted_50_half\":    [1/50] * 26 + [Fake(1/50, attack_type='inverted')] * 24,\n",
    "}\n",
    "VALIDATION_FRACTIONS = [0.2]  # put aside 20% of the records for validation. Not gonna sweep on this\n",
    "\n",
    "\n",
    "DATA_10PERCENT = \"kddcup.data_10_percent\"\n",
    "DATA_FULL = \"kddcup.data\"\n",
    "DATASET_NAME = DATA_FULL  # Should be DATA_10PERCENT or DATA_FULL\n",
    "CUSTOM_TEST = True  # True to split DATASET_NAME into 2/3 train/validate 1/3 test; False to use all of DATASET_NAME for the clients and DATA_CORRECTED for test\n",
    "DATA_FOLDER = f\"/path/to/data/root/folder/{DATASET_NAME}/\"\n",
    "DATASET_PATH = os.path.join(DATA_FOLDER, DATASET_NAME)\n",
    "DATA_CORRECTED = os.path.join(DATA_FOLDER, \"corrected\")\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_PATH):\n",
    "    zipped_file = DATASET_PATH + \".gz\"\n",
    "    if not os.path.exists(zipped_file):\n",
    "        !wget -O \"{zipped_file}\" http://kdd.ics.uci.edu/databases/kddcup99/{DATASET_NAME}.gz\n",
    "    !gunzip \"{zipped_file}\"\n",
    "if not os.path.exists(DATA_CORRECTED):\n",
    "    zipped_file = os.path.join(DATA_FOLDER, \"corrected.gz\")\n",
    "    if not os.path.exists(zipped_file):\n",
    "        !wget -O \"{zipped_file}\" http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\n",
    "    !gunzip \"{zipped_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = kdd99_csv_to_dataframe(DATASET_PATH)\n",
    "df_corrected = kdd99_csv_to_dataframe(DATA_CORRECTED)\n",
    "kdd99_df_to_dataset(df_normal).save(os.path.join(DATA_FOLDER, \"normal.dat\"))\n",
    "kdd99_df_to_dataset(df_corrected).save(os.path.join(DATA_FOLDER, \"corrected_all.dat\"))\n",
    "df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_normal.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all new attacks from the corrected dataset\n",
    "known_corrected_df_slices = []\n",
    "for attack_name, attack_type in ATTACK_CLASSIFICATIONS:\n",
    "    known_corrected_df_slices.append(df_corrected[df_corrected[\"attack_type\"] == attack_name])\n",
    "known_corrected_df = pd.concat(known_corrected_df_slices)\n",
    "kdd99_df_to_dataset(known_corrected_df).save(os.path.join(DATA_FOLDER, \"corrected_known.dat\"))\n",
    "known_corrected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal_shuffle = df_normal.sample(frac=1)\n",
    "df_normal_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tv = df_normal_shuffle.iloc[:int(2/3*len(df_normal_shuffle))]\n",
    "df_test = df_normal_shuffle.iloc[int(2/3*len(df_normal_shuffle)):]\n",
    "kdd99_df_to_dataset(df_tv).save(os.path.join(DATA_FOLDER, \"normal_tv.dat\"))\n",
    "kdd99_df_to_dataset(df_test).save(os.path.join(DATA_FOLDER, \"normal_test.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_df(df: pd.DataFrame, start_i: int, fraction: Union[float, Fake]):\n",
    "    if isinstance(fraction, float):\n",
    "        num_records = int(fraction * len(df))\n",
    "        new_df = df.iloc[start_i:start_i+num_records]\n",
    "        new_start_i = start_i + num_records\n",
    "        return (new_start_i, new_df)\n",
    "    else:\n",
    "        assert isinstance(fraction, Fake)\n",
    "        num_records = int(fraction.fraction * len(df))\n",
    "        real_df = df.iloc[start_i:start_i+num_records]\n",
    "        if fraction.attack_type == \"random\":\n",
    "            fake_data = real_df.to_numpy(copy=True)\n",
    "            for i, column in enumerate(real_df.columns):\n",
    "                size = len(real_df)\n",
    "                try:\n",
    "                    mean = real_df[column].mean()\n",
    "                    std = real_df[column].std()\n",
    "                    fake_data[:, i] = np.random.normal(mean, std, size=size)\n",
    "                except TypeError:  # it's discrete\n",
    "                    names = []\n",
    "                    counts = []\n",
    "                    for name, count in real_df[column].value_counts().items():\n",
    "                        assert isinstance(name, str)\n",
    "                        names.append(name)\n",
    "                        counts.append(count)\n",
    "                    np_counts = np.array(counts, dtype=np.float)\n",
    "                    np_counts /= np.sum(np_counts)\n",
    "                    fake_data[:, i] = np.random.choice(names, size=size, replace=True, p=np_counts)\n",
    "                    \n",
    "            fake_df = pd.DataFrame(data=fake_data, columns=real_df.columns).infer_objects()\n",
    "        else:\n",
    "            assert fraction.attack_type == \"inverted\"\n",
    "            fake_df = real_df.copy(deep=True)\n",
    "            fake_df.loc[fake_df['attack_type'] != 'normal.', 'attack_type'] = 'attack.'\n",
    "            fake_df.loc[fake_df['attack_type'] == 'normal.', 'attack_type'] = 'FAKE'\n",
    "            fake_df.loc[fake_df['attack_type'] == 'attack.', 'attack_type'] = 'normal.' \n",
    "            fake_df.loc[fake_df['attack_type'] == 'FAKE', 'attack_type'] = 'attack.'\n",
    "        new_start_i = start_i + num_records\n",
    "        return (new_start_i, fake_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CUSTOM_TEST:\n",
    "    df_shuffle = df_tv\n",
    "    print(\"Using 66% of noraml data\")\n",
    "else:\n",
    "    df_shuffle = df_normal_shuffle\n",
    "    print(\"Using 100% of normal data\")\n",
    "print(\"Length of dataset\", len(df_shuffle))\n",
    "for split_name, split in SPLITS.items():\n",
    "    i = 0\n",
    "    for validation_size in VALIDATION_FRACTIONS:\n",
    "        split_folder = os.path.join(DATA_FOLDER, f\"split_{split_name}_validation_fraction_{validation_size}\")\n",
    "        shutil.rmtree(split_folder, ignore_errors=True)\n",
    "    for client_i, fraction in enumerate(split):\n",
    "        assert i < len(df_shuffle)\n",
    "        i, new_df = slice_df(df_tv, i, fraction)\n",
    "        for validation_size in VALIDATION_FRACTIONS:\n",
    "            train_size = int((1 - validation_size) * len(new_df))\n",
    "            train_df = new_df[:train_size]\n",
    "            validation_df = new_df[train_size:]\n",
    "            split_folder = os.path.join(DATA_FOLDER, f\"split_{split_name}_validation_fraction_{validation_size}\")\n",
    "            client_folder = os.path.join(split_folder, f\"client_{client_i}\")\n",
    "            os.makedirs(client_folder)\n",
    "            print(\"Saving to \", client_folder)\n",
    "            kdd99_df_to_dataset(train_df).save(os.path.join(client_folder, \"train.dat\"))\n",
    "            kdd99_df_to_dataset(validation_df).save(os.path.join(client_folder, \"validation.dat\"))\n",
    "            kdd99_df_to_dataset(new_df).save(os.path.join(client_folder, \"score.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
